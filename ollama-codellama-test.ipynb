{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PYTHON]\n",
      "def print_hello_world():\n",
      "    print(\"Hello, World!\")\n",
      "[/PYTHON]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"codellama:13b-instruct\")\n",
    "\n",
    "print(llm.invoke(\"\"\"#Write python function to print hellow world\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  This code is a method of a class that builds a list of messages to be sent as part of a conversation. The method takes in the current question being asked and returns a list of dictionaries, each representing a message with a \"role\" (either \"system\", \"user\", or \"assistant\") and a \"content\".\n",
      "\n",
      "The method starts by creating an empty list for the messages and adding a system message to it. The system message is created by calling the `build_context` method of the class, which returns a string representing the current state of the conversation.\n",
      "\n",
      "Next, the method iterates over the items in the `_conv_dict` attribute of the class, which seems to be a dictionary mapping questions to responses and other metadata. For each item, it checks if the response is complete (not skipped) and if so, appends a user message to the list with the previous question concatenated with the current question. It then appends an assistant message with the response.\n",
      "\n",
      "If the response was not complete, the method sets the previous question to the current question and moves on to the next item in the `_conv_dict`. Finally, if there is a current question that has not been processed yet (i.e., it's not in the `_conv_dict`), the method adds a user message with the concatenated previous question and current question to the list.\n",
      "\n",
      "The method returns the list of messages at the end.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"\"\"\n",
    "What does this code do, please explain\n",
    "                 \n",
    "                 def buildMessages(self, current_question : str) -> list[dict[str, str]] :\n",
    "        retMessages : list[dict[str, str]] = []\n",
    "        retMessages.append({\"role\": \"system\", \"content\": self.build_context()})\n",
    "        prevQuestion = \"\"\n",
    "        for question, (response, isComplete, skipped) in self._conv_dict.items():\n",
    "            if isComplete and not skipped:\n",
    "                retMessages.append({\"role\": \"user\", \"content\": prevQuestion + question})\n",
    "                retMessages.append({\"role\": \"assistant\", \"content\": response })\n",
    "                prevQuestion = \"\"\n",
    "            else:\n",
    "                # The response was not complete, so mark the question as the previous question\n",
    "                prevQuestion = question\n",
    "        \n",
    "        question_to_add = prevQuestion + current_question if current_question not in self._conv_dict else \"\"\n",
    "\n",
    "        if question_to_add:\n",
    "            retMessages.append({\"role\": \"user\", \"content\": question_to_add})\n",
    "        \n",
    "        #print(\"Messages:\", retMessages)\n",
    "        return retMessages\n",
    "\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
